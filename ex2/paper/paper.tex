%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%%
%% IMPORTANT NOTICE:
%%
%% For the copyright see the source file.
%%
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%%
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%%
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[10pt,sigconf,letterpaper,dvipsnames]{acmart}
\acmYear{2019}
\copyrightyear{2019}
\acmConference{CoNEXT '19}{December 9-12, 2019}{Orlando, Florida, USA}

\usepackage[utf8]{inputenc}

\usepackage{multirow}
\usepackage{blindtext}
\usepackage{soul}

% Acronyms
\usepackage[nomain, toc, acronym]{glossaries}

%\settopmatter{printacmref=false} % Removes citation information below abstract
%\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column

\newcommand\note[2]{{\color{#1}#2}}
\newcommand\todo[1]{{\note{red}{TODO: #1}}}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Bricking up and overgrowing backdoors in Intrusion Detection Systems}

%% GLOSSARY

\newacronym{dl}{DL}{Deep Learning}
\newacronym{ml}{ML}{Machine Learning}
\newacronym{cnn}{CNN}{Convolutional Neural Network}
\newacronym{ale}{ALE}{Accumulated Local Effects}
\newacronym{mui}{MuI}{Model under Investigation}
\newacronym{ttl}{TTL}{Time-to-Live}
\newacronym{pdp}{PDP}{Partial Dependence Plot}
\newacronym{ale}{ALE}{Accumulated Local Effects}
\newacronym{ids}{IDS}{Intrusion Detection System}
\newacronym{nids}{NIDS}{Network Intrusion Detection System}
\newacronym{rf}{RF}{Random Forest}

\begin{abstract}
Interest in poisoning attacks and backdoors recently resurfaced for \gls{dl} applications. Several successful defense mechanisms have been recently proposed for Convolutional Neural Networks, for example in the context of autonomous driving. We show that, surprisingly, common defense mechanisms fail utterly to remove backdoors in \gls{dl} for \glspl{ids}. However, we find that visualization approaches can aid in identifying a backdoor independent of the used classifier. Finally, we conceive pruning-based approaches to remove backdoors for decision trees and random forests and demonstrate their effectiveness for several network security datasets.
\end{abstract}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Maximilian Bachl}
\affiliation{\institution{Technische Universit채t Wien}}
%\authornote{Both authors contributed equally to this research.}
\email{maximilian.bachl@tuwien.ac.at}

\author{Alexander Hartl}
\affiliation{\institution{Technische Universit채t Wien}}
%\authornotemark[1]
\email{alexander.hartl@tuwien.ac.at}

\author{Joachim Fabini}
\affiliation{\institution{Technische Universit채t Wien}}
\email{joachim.fabini@tuwien.ac.at}

\author{Tanja Zseby}
\affiliation{\institution{Technische Universit채t Wien}}
\email{tanja.zseby@tuwien.ac.at}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Hartl and Bachl}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
%\begin{abstract}
%  A clear and well-documented \LaTeX\ document is presented as an
%  article formatted for publication by ACM in a conference proceedings
%  or journal publication. Based on the ``acmart'' document class, this
%  article presents and explains many of the common variations, as well
%  as many of the formatting elements an author may use in the
%  preparation of the documentation of their work.
%\end{abstract}

\settopmatter{printfolios=true}
\maketitle

\section{Introduction}
The detection of attacks in data networks is a fundamental task in data security. Due to the considerable amount of data which has to be analyzed the use of machine learning techniques for this purpose seems natural and is increasingly deployed.

For research, the invention and assessment of techniques for network anomaly detection poses many challenges. To implement such a system, a considerable number of features can be extracted from network data which might be beneficial for anomaly detection and it is not always clear which features are actually important.

For the training of models usually datasets are used which have been generated artificially in a controlled test environment. As a downside of this approach, it is unclear whether a machine learning model learns to classify based on characteristics that are inherent to the attacks which should be detected, or rather learns to classify based on patterns that were unintentionally created during dataset generation.

For a well-performing network anomaly detection technique it is therefore of utmost importance to study which patterns the technique looks at to distinguish attack traffic from normal traffic, and question if these explanations match with expert knowledge.

% TODO: think meghdouri_analysis_2018 didnt use this
In this document, we redo parts of a recent paper~\cite{meghdouri_analysis_2018} which bases on the CIC-IDS-2017 dataset \cite{sharafaldin_toward_2018} for evaluating the performance of several feature vectors and machine learning techniques for accurate anomaly detection. We use explainability methods for investigating if the decisions the anomaly detectors untertake are reasonable.

Furthermore, we add a backdoor to the trained model and show that attack detection can efficiently be bypassed if the attacker had the ability to modify training data. Finally, we apply the same explainability methods to the backdoored model and show how such attack attempts might be recognized before any harm is done.


\begin{table}[b]
\caption{Detection performance results.} \label{tab:performance_results}
\begin{tabular}{l r r} \toprule
& Random Forest & Deep Learning \\ \midrule
Accuracy	&	0.9974 $\pm$ 0.0001	&	0.9970 $\pm$ 0.0002	\\
Precision	&	0.9965 $\pm$ 0.0001	&	0.9979 $\pm$ 0.0001	\\
Recall	&	0.9931 $\pm$ 0.0002	&	0.9903 $\pm$ 0.0007	\\
F1	&	0.9948 $\pm$ 0.0001	&	0.9941 $\pm$ 0.0004	\\
Youden	&	0.9920 $\pm$ 0.0002	&	0.9896 $\pm$ 0.0007	\\
\bottomrule
\end{tabular}
\end{table}

\section{Related Work}

\section{Experimental Setup} \label{sec:ml_approaches}
\subsection{The Backdoor}
Creating a model for anomaly detection in network traffic is a challenging task which involves massive datasets and significant amounts of computation power. In a practical deployment, it is therefore reasonable to assume that the training is done by a security company marketing either a complete anomaly detection system or just a model usable by an anomaly detection software which might even be open-source.

If we have to question if such a security company can be trusted under all circumstances, the problem occurs that the security company might have tried to implement measures to circumvent reliable attack detection of their attack detection system.

For this reason, we now add a backdoor to our deep learning and random forest models and investigate if detection of malicious training can be detected by applying explainability methods to the training models.

Hence, for this task we first had to find a pattern usable as a backdoor in feature vectors. On one hand, this pattern has to be detectable reliably by the anomaly detection system, one the other hand it must be possible to generate real network traffic which translate into feature vectors exhibiting these patterns.

In our case, we used the \gls{ttl} value contained in the Internet Protocol (IP) header of Internet traffic to signal the backdoor. The \gls{ttl} is used for mitigating problems due to routing loops in IP networks. In our situation, it is reasonable to assume that the \gls{ttl} field remains constant for all packets in one traffic flow. In particular this assumption was perfectly reflected by the examined dataset, as there were no traffic flows with a non-zero standard deviation of the \gls{ttl} value.

Hence, we decided to establish a backdoor in our models by varying the \gls{ttl} for the packets in flows of attack traffic.  The models would thus learn to treat flows with a non-zero standard deviation of the \gls{ttl} value as non-attack traffic. Specifically we implement the backdoor by incrementing the \gls{ttl} of the first packet by one if its \gls{ttl} is smaller than 128 and decrementing it by 1 if is larger. This results in a tiny standard deviation of the \gls{ttl} (which would be otherwise undefined or zero) as well as in changed max, min and mean.
\subsection{Datasets}
Several datasets for the purpose of building and evaluating IDSs have been developed. However, as pointed out in \cite{gharib_evaluation_2016}, there are numerous requirements that have to be met for a dataset to provide realistic performance benchmarks in this context.

In this research, we use the UNSW-NB15~\cite{moustafa_unsw-nb15:_2015} and the CIC-IDS-2017~\cite{sharafaldin_toward_2018} datasets, which were developed by two independent institutions and are both freely available on the Internet, guaranteeing reproducibility of our results.

The UNSW-NB15 dataset~\cite{moustafa_unsw-nb15:_2015} was created by researchers of the University of New South Wales to overcome common problems due to outdated datasets. Network captures containing over 2 million flows of normal traffic and various types of attacks are provided together with a ground truth file. Attack traffic includes exploits, fuzzers, reconnaissance attacks, DoS attacks, shellcode, analysis attacks, backdoors and worms.

The CIC-IDS-2017 dataset~\cite{sharafaldin_toward_2018} was created by the Canadian Institute of Cybersecurity to provide an alternative to existing datasets which are found to exhibit several shortcomings. The provided raw network captures contain over 2.3 million flows, containing both normal traffic and attacks, which can be coarsely classified into DoS attacks, infiltration attacks, web attacks, brute force and scanning attacks.

We base our analysis on the CAIA~\cite{williams_preliminary_2006} feature vector as formulated in~\cite{meghdouri_analysis_2018}, which thus includes the used protocol, flow duration, packet count and the total number of transmitted bytes, the minimum, maximum, mean and standard deviation of packet length and inter-arrival time and the number of packets with specific TCP flags set. All features except protocol and flow duration are evaluated for forward and backward direction separately. The goal of this research is to investigate the possibility of poisoning attacks. We therefore also include the minimum, maximum and standard deviation of TTL values in our feature vector as an attractive carrier for exploitation as a backdoor. % TODO: justify use of TTL better

We used the go-flows~\cite{vormayr_cn-tu/go-flows_2019} flow exporter for extracting these features from the raw capture files, applied Z-score normalization to process the data and used 3-fold cross validation for evaluating the models' performances.

% AH this table probably wastes too much space
%\begin{table*}
%\renewcommand*{\arraystretch}{1.2} % TODO: better order
%\caption{Flow features used for attack detection.}
%\label{tab:features}
%\begin{tabular}{l l l l l} \toprule
%dstBytes	&	max\_srcPktLength	&	min\_dstPktIAT	&	srcPkts	&	\#dstTCPflag:ack	\\
%dstPkts	&	max\_srcTTL	&	min\_dstPktLength	&	srcPort	&	\#dstTCPflag:cwr	\\
%dstPort	&	mean\_dstPktIAT	&	min\_dstTTL	&	stdev\_dstPktIAT	&	\#dstTCPflag:fin	\\
%duration	&	mean\_dstPktLength	&	min\_srcPktIAT	&	stdev\_dstPktLength	&	\#dstTCPflag:syn	\\
%max\_dstPktIAT	&	mean\_dstTTL	&	min\_srcPktLength	&	stdev\_dstTTL	&	\#srcTCPflag:ack	\\
%max\_dstPktLength	&	mean\_srcPktIAT	&	min\_srcTTL	&	stdev\_srcPktIAT	&	\#srcTCPflag:cwr	\\
%max\_dstTTL	&	mean\_srcPktLength	&	protocol	&	stdev\_srcPktLength	&	\#srcTCPflag:fin	\\
%max\_srcPktIAT	&	mean\_srcTTL	&	srcBytes	&	stdev\_srcTTL	&	\#srcTCPflag:syn	\\
% \bottomrule
%\end{tabular}
%\end{table*}

\subsection{Machine Learning Techniques}
\subsubsection{Deep Learning}
We used PyTorch to build a neural network classifier. For this we used 5 fully connected layers with 512 neurons each and applied ReLU after each layer as well as dropout with a probability of 0.2. We use the binary cross entropy loss function for the classification task itself.

\subsubsection{Random Forests}
In addition to deep learning we trained a random forest classifier to detect attacks in network traffic. We used the RandomForestClassifier implementation from scikit-learn for this task and used 100 estimators for training each random forest.


\subsection{Performance Results}
Table~\ref{tab:performance_results} shows performance results for both deep learning and random forests. Our results are consistent with previous work like, e.g., \cite{meghdouri_analysis_2018}.

\section{Remedies for Poisoning Attacks}
We now investigate several techniques which might be used to prevent security vulnerabilities due to pretrained models. In this respect the ability to detect a backdoor and the ability to remove a backdoor from the trained model can be considered as equally effective since one often has the option to fall back to a model obtained from a different source in case a model looks suspicious.

\subsection{Explainability Plots} \label{sec:plots}
A major problem for the deployment of ML is the unpredictability and the lack of understanding of the decisions an ML models takes. For this reason, a number of methods has been proposed in the recent years aiming to visualize and explain a non-interpretable ML model's decisions.

Applied to the present problem, we can pick up ideas from \glspl{pdp} and \gls{ale} plots, not only for identifying backdoors in the \gls{mui}, but also for finding wrong decisions it would take due to flawed training data.

%Several graphs have been proposed for visualizing feature dependence of non-interpretable machine learning models~\cite{goldstein2015peeking, friedman_greedy_2001, apley2016visualizing}.
%In this section, we introduce the graphs which we will use for interpreting our models' outcomes which are not available in real-world situations.
\subsubsection{Partial Dependence Plots}
\glspl{pdp} were proposed in \cite{friedman_greedy_2001} and visualize dependence of a model's predictions by plotting the \gls{mui}'s prediction for a modified dataset for which the feature's value has been fixed to a certain value and computing the an average over the dataset.

If we denote by $\boldsymbol X \in \mathbb R ^n$ a random vector drawn from the feature space and by $f(\boldsymbol X) \in [0,1]$ the  prediction function, the \gls{pdp} for the $i$th feature $X_i$ can be expressed as
\begin{equation}
\text{PDP}_i(y) = \mathbb E_{\boldsymbol X}\left(f(X_1,\ldots,X_{i-1},y,X_{i+1},\ldots X_n)\right) . % \int _{\mathbb R^n} Predict(x_1,\ldots,x_{i-1},y,x_{i+1},\ldots x_n) f(\boldsymbol x) d\boldsymbol x .
\end{equation}
Empirically, we can approximate the distribution of the feature space using the distribution of observed samples. Hence, at a given point $y,$ the \gls{pdp} can be found by setting the corresponding value of all samples in the dataset to $y$ and averaging over the predictions of the resulting modified dataset.

\subsubsection{Accumulated Local Effects}
Due to feature dependence it is very likely that in the feature space areas exist which have a very low probability to occur. Since a model is trained with real, observed data, the training set therefore does not include samples for these areas, which causes the model's predictions to become indeterminate for these areas. This poses a problem when considering these predictions for computing \glspl{pdp}.

In an attempt to overcome this problem, it is possible to only consider samples which are likely to occur for certain feature values, i.e. to consider the conditional distribution of remaining features, for computing explainability graphs. This results the concept for \gls{ale} plots.

For the $i$th feature $X_i$, the \gls{ale} plot ALE$_i(y)$ can be defined differentially as



\begin{equation}
\frac{d}{dy} \text{ALE}_i (y) = \mathbb E_{\boldsymbol X | X_i}\left(\frac{d}{dy} f(X_1,\ldots,X_{i-1},y,X_{i+1},\ldots X_n) | X_i=y\right)
\end{equation}
%\todo{I think that there are some minor problems with $y$ and $X_i$ in the formula.}

To combat ambiguity of this definition, we force ALE$_i(x)$ to have zero mean on the domain of $X_i$.

For empirical evaluation, we approximate the conditional distributions of $\boldsymbol X$ by averaging over samples for which $X_i \approx y$.

When computing \gls{ale} plots, we experienced the problem of empty intervals. If there are intervals  that do not contain any values, the usual definition which takes values between the interval's boundaries for estimating the conditional probability density for feature values doesn't work.

For this reason, we modified this definition to instead use the closest 10 samples to the interval's center for estimating the distribution.

\subsubsection{Identifying Backdoors}
Backdoors can be identified by computing \gls{pdp} and or \gls{ale} plots for the \gls{mui} and investigating if regions exist which the \gls{mui} behaves counter-intuitive. For our \gls{rf} model, \autoref{fig:pdp_ttl} shows the \gls{pdp} for the \gls{ttl} value in forward direction. 
\subsection{Pruning Techniques}
\subsubsection{Neural Network Pruning}
\subsubsection{Random Forest Pruning}
For intrusion detection decision tree based models or random forests of decision trees have been shown to usually achieve higher accuracy than multilayer perceptrons \todo{citation needed} and moreover they are generally faster to train and possibly more interpretable. For \glspl{nids} we thus consider the removal of backdoors from decision tree based models to be a more pressing issue than from multilayer perceptrons. 

The defense mechanism proposed by \cite{biggio_bagging_2011} uses bagging to try to minimize the influence of a backdoor that is introduced in the training dataset via poisoning and is applicable to decision trees. However, this approach cannot protect if a trained model is obtained from another (untrusted) party in which the other party might potentially have introduced a backdoor. 

We thus envision a defense approach that can be used for similar use cases like the mechanisms recently proposed for \glspl{cnn}.

\begin{figure*}[h]
\includegraphics[width=\textwidth]{pruning_example.pdf}

\caption{Example of a decision tree being pruned. To the left of each leaf we show \textcolor{MidnightBlue}{the number of times a leaf was used} by validation samples and its \textcolor{Fuchsia}{depth} in the decision tree to the right of each leaf. We also highlight leaves that indicate that a sample is \protect\sethlcolor{green} \hl{harmless}  (leaves with a 0) because these are the ones our algorithm considers as candidates for pruning. The leaf that is going to be cut in each step has a dashed connection to its parent. This decision tree is not functional anymore in the end because all its leaves indicate that a sample is an attack. However, the decision trees that we prune usually have several thousands of leaves and we never prune until the very end and thus this case of a degenerate decision tree is not problematic.}
\label{fig:pruningExample}
\end{figure*}


\subsection{Neural Network Fine-Tuning}

\section{Discussion}


\section{Conclusions}

We used a dataset for network anomaly detection and extracted features using an enhanced version of the CAIA feature vector. Then we trained two non-interpretable classifiers on the resulting data. The resulting random forest as well as the deep learning model performed very well on the dataset. As the next step we implemented a backdoor into the dataset. For this we had to find a feature that can actually be modified easily by an attacker. We found that the \gls{ttl} fulfills these criteria and implemented a backdoor in it by modifying the  \gls{ttl} of selected packets. The resulting backdoor was successfully detected by the classifier in nearly all cases. Furthermore, the accuracy of the classifier on the original dataset did not change significantly. However, the backdoor was clearly visible by inspecting the plots yielded by explainability methods. It follows that designers of network \glspl{ids} must carefully choose which feature vectors to use and to keep them small in order to reduce the attack surface for backdoors. Also, every \gls{ids} in production use should be thoroughly investigated with explainability methods to detect possible backdoors.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\subsection{Interpretation}
\subsubsection{PDPs}

\begin{figure*}[p]
\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/sourceTransportPort_nn.pdf}
\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/sourceTransportPort_rf.pdf}

\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/apply(mean(ipTTL),forward)_nn.pdf}
\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/apply(mean(ipTTL),forward)_rf.pdf}

\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/apply(stdev(ipTotalLength),forward)_nn.pdf}
\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/apply(stdev(ipTotalLength),forward)_rf.pdf}

\caption{Examples for PD plots for deep learning (left side) and random forest (right side).}
\label{fig:pdp}
\end{figure*}

We generated PD plots for all features contained in our extracted feature vector. Fig.~\ref{fig:pdp} shows examples for some features for the different folds. Comparing plots generated for our deep learning and for our random forest approach, respectively, the two machine learning approaches show the difference that deep learning PDPs are smooth, while random forest PDPs fluctuate heavily. Considering the functioning of neural networks and decision trees, this is an expected result.

Considering the explanations the PD plots provide, consistency between deep learning and random forest can be observed for some features, while for other features both methods provide substantially different explanations. For example, the two upper plots in Fig.~\ref{fig:pdp} exhibit a very similar behavior, while the lower plots show remarkable differences between both methods. An additional observation is, however, that random forests show substantial differences between the folds.

Considering these observations, it is likely that most samples from the dataset have very small values of \textit{stdev\_srcPktLength}, so that for neither deep learning nor random forests it is important to predict higher values of \textit{stdev\_srcPktLength}.

The topmost plots in Fig.~\ref{fig:pdp} show the PDP for source ports, which intuitively provides important information for attack detection. Indeed, the plots show that predictions vary substantially with this feature. Furthermore, attack traffic mainly uses higher (unprivileged) ports.

More importantly, however, the deep spike around port 50000 catches the eye immediately. In the network that was used for generating the dataset, most likely massive amounts of legitimate traffic originating from this port, existed.
Since this is a characteristic which is very specific to the used dataset, it is questionable if this expresses desired behaviour.

Most importantly, however, the \textit{mean\_srcTTL} feature depicted in Fig.~\ref{fig:pdp} shows that both deep learning and random forest learn to consider very specific TTL values for their decisions. This points out that attack traffic was generated from a very specific network with specific TTL values throughout dataset generation.
By no means can this behaviour generalize to network traffic in real deployments.

\subsubsection{ALE plots}

\begin{figure*}[p]
\includegraphics[width=0.48\textwidth]{../ale_CAIA_backdoor_17/sourceTransportPort_nn.pdf}
\includegraphics[width=0.48\textwidth]{../ale_CAIA_backdoor_17/sourceTransportPort_rf.pdf}

\includegraphics[width=0.48\textwidth]{../ale_CAIA_backdoor_17/apply(mean(ipTTL),forward)_nn.pdf}
\includegraphics[width=0.48\textwidth]{../ale_CAIA_backdoor_17/apply(mean(ipTTL),forward)_rf.pdf}

\includegraphics[width=0.48\textwidth]{../ale_CAIA_backdoor_17/apply(stdev(ipTotalLength),forward)_nn.pdf}
\includegraphics[width=0.48\textwidth]{../ale_CAIA_backdoor_17/apply(stdev(ipTotalLength),forward)_rf.pdf}

\caption{Examples for ALE plots for deep learning (left side) and random forest (right side).}
\label{fig:ale}
\end{figure*}

In addition to PD plots, we generated ALE plots for all features in our feature vectors. Fig.~\ref{fig:ale} depicts plots for the same features as presented in Fig.~\ref{fig:pdp}.

As described in Section~\ref{sec:plots}, ALE plots might provide benefits in explaining the models' decisions and, hence, seem particularly important.

Plots depicted in Fig.~\ref{fig:ale} show a certain similarity to the PD plots in Fig.\ref{fig:pdp}. However, as apparant at the \textit{srcPort}, explanations from PD and ALE plots might also deviate substantially \todo{Are we sure that this is not a bug?}.

An important observation for ALE plots particularly for random forests is that influence of TTL values is even more distinctive. In fact, judging from ALE plots the random forest seems to almost exclusively consider the TTL for detecting attacks. This emphasizes the problem described already for PDPs.

\section{Implementing a Backdoor}


\subsection{Detection}


% AH This figures currently serve as placeholder/example. TOOD: decide what to plot exactly
\begin{figure}[t]
\includegraphics[width=\columnwidth]{../prune_CAIA_backdoor_15/prune.pdf}
\caption{Performance metrics throughout the pruning process.}
\end{figure}

% AH This figures currently serve as placeholder/example. TOOD: decide what to plot exactly
\begin{figure}[t]
\includegraphics[width=\columnwidth]{../prune_CAIA_backdoor_15/prune.pdf}
\caption{Performance metrics throughout the pruning process.}
\end{figure}

\begin{figure}[t]
\includegraphics[width=\columnwidth]{../prune_CAIA_backdoor_17/{prune_1.00_nn_0_bd}.pdf}
\caption{Correlation coefficient of neuron activation with backdoor usage throughout the pruning process.}
\end{figure}


\begin{figure*}[p]
%\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/sourceTransportPort_nn_bd.pdf}
%\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/sourceTransportPort_rf_bd.pdf}

\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/apply(stdev(ipTTL),forward)_nn.pdf}
\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/apply(stdev(ipTTL),forward)_rf.pdf}

\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/apply(stdev(ipTTL),forward)_nn_bd.pdf}
\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/apply(stdev(ipTTL),forward)_rf_bd.pdf}

%\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/apply(stdev(ipTotalLength),forward)_nn_bd.pdf}
%\includegraphics[width=0.48\textwidth]{../pdp_CAIA_backdoor_17/apply(stdev(ipTotalLength),forward)_rf_bd.pdf}

\caption{Examples for PD plots for deep learning (left side) and random forest (right side). The top row is from a classifier without backdoor while at the bottom there is a backdoor.}
\label{fig:pdp_backdoor}
\end{figure*}

\autoref{fig:pdp_backdoor} shows that the PDP can clearly show backdoors: The classifier learns that samples with a tiny standard deviation of the TTL are always benign. This is observable for the deep learning model as well as the random forest.

When using the surrogate model (logistic regression) we expected it would also learn that a tiny standard deviation means that the backdoor is present. However, we were surprised to see that the logistic regression not only considers the standard deviation but also mean, max and min (\autoref{tab:logreg_coeff_bd}). Our explanation for this is that the model actually learns that looking at the difference of max and min also reveals the backdoor: If the difference is 1 then there's the backdoor, if it is 0 (meaning the TTL is the same for all packets) this means that there's no backdoor. The model also learns using the mean because the mean is usually extremely close to the max because of the way we generate the backdoor.

\begin{table}
\caption{Detection performance results for models with backdoor.}
\label{tab:performance_results_bd}
\begin{tabular}{l r r} \toprule
& Random Forest & Deep Learning \\ \midrule
Accuracy	&	0.9986 $\pm$ 0.0004	&	0.9974 $\pm$ 0.0001		\\
Precision	&	0.9981 $\pm$ 0.0005	&	0.9986 $\pm$ 0.0004		\\
Recall	&	0.9964 $\pm$ 0.0009	&	0.9912 $\pm$ 0.0004		\\
F1	&	0.9972 $\pm$ 0.0007	&	0.9949 $\pm$ 0.0002		\\
Youden	&	0.9957 $\pm$ 0.0011	&	0.9907 $\pm$ 0.0003		\\
\bottomrule
\end{tabular}
\end{table}


\begin{table}
\caption{Efficacy of the backdoor.}
\begin{tabular}{l r r} \toprule
 & Random Forest & Deep Learning \\ \midrule
Forward & 1 & 0,9999 $\pm$ 0,0001 \\
Backward & 1 & 0,9999 $\pm$ 0,0000 \\
\bottomrule
\end{tabular}
\end{table}








\section{Misclassification Examples}
\autoref{tab:misclassified_instances} shows the total number of attack samples in the dataset of each attack class as well as what we call \textit{false positives} and \textit{false negatives}. The false negatives are simply the number of samples of each class that were falsely classified as benign even though they are actually attacks.
The false positives are the classes of the attack samples that are the closest to a benign sample that was wrongly classified as being an attack.

\begin{table*}
\caption{Misclassified instances of the first fold: \textit{false positives} shows the class of the attack sample that is closest to the misclassified benign sample; \textit{false negatives} shows how many attack samples of each class were falsely classified as benign.}
\label{tab:misclassified_instances}
\begin{tabular}{l l l l}
\toprule
Attack type & total number & false positives & false negatives \\
\midrule
Botnet:ARES  & 275 & 13 & 6 \\
Brute Force:FTP-Patator & 91 & 2 & 81 \\
Brute Force:SSH-Patator & 870 & 67 & 1 \\
DDoS:LOIT & 31887 & 0 & 29 \\
DoS / DDoS:DoS GoldenEye & 2490 & 2 & 1 \\
DoS / DDoS:DoS Hulk & 77998 & 4 & 1 \\
DoS / DDoS:DoS Slowhttptest & 1385 & 142 & 4 \\
DoS / DDoS:DoS slowloris & 1310 & 2 & 1 \\
DoS / DDoS:Heartbleed & 2 & 0 & 2 \\
Infiltration:Dropbox download & 25587 & 87 & 1783 \\
PortScan:PortScan - Firewall off & 53341 & 8 & 18 \\
PortScan:PortScan - Firewall on & 129 & 0 & 7 \\
Web Attack:Sql Injection & 6 & 0 & 1 \\
Web Attack:XSS & 233 & 69 & 100 \\
\bottomrule
\end{tabular}
\end{table*}



\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
