
@article{fang_learning_2017,
	title = {Learning how to {Active} {Learn}: {A} {Deep} {Reinforcement} {Learning} {Approach}},
	shorttitle = {Learning how to {Active} {Learn}},
	url = {http://arxiv.org/abs/1708.02383},
	abstract = {Active learning aims to select a small subset of data for annotation such that a classiﬁer learned on the data is highly accurate. This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets. To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic. Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages. We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.},
	language = {en},
	urldate = {2020-01-14},
	journal = {arXiv:1708.02383 [cs]},
	author = {Fang, Meng and Li, Yuan and Cohn, Trevor},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.02383},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: To appear in EMNLP 2017},
	file = {Fang et al. - 2017 - Learning how to Active Learn A Deep Reinforcement.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/2X43X5UN/Fang et al. - 2017 - Learning how to Active Learn A Deep Reinforcement.pdf:application/pdf}
}

@misc{noauthor_active_nodate,
	title = {Active learning (machine learning) - {Wikipedia}},
	url = {https://en.wikipedia.org/wiki/Active_learning_(machine_learning)},
	urldate = {2020-01-14},
	file = {Active learning (machine learning) - Wikipedia:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/L6D29GB4/Active_learning_(machine_learning).html:text/html}
}

@article{mnih_asynchronous_2016,
	title = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	urldate = {2020-01-16},
	journal = {arXiv:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv: 1602.01783},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/ZYPU4IR5/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learni.pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/27VWHXNW/1602.html:text/html}
}

@article{wassermann_ral_2019,
	title = {{RAL} - {Improving} {Stream}-{Based} {Active} {Learning} by {Reinforcement} {Learning}},
	abstract = {One of the main challenges associated with supervised learning under dynamic scenarios is that of periodically getting access to labels of fresh, previously unseen samples. Labeling new data is usually an expensive and cumbersome process, while not all data points are equally valuable. Active learning aims at labeling only the most informative samples to reduce cost. In this paradigm, a learner can choose from which new samples it wants to learn, and can obtain the ground truth by asking an oracle for the corresponding labels. We introduce RAL – Reinforced stream-based Active Learning –, a new active-learning approach, coupling stream-based active learning with reinforcement-learning concepts. In particular, we model active learning as a contextual-bandit problem, in which rewards are based on the usefulness of the system’s querying behavior. Empirical evaluations on multiple datasets conﬁrm that RAL outperforms the state of the art, both by improving learning accuracy and by reducing the number of requested labels. As an additional contribution, we release RAL as an open-source Python package to the machine-learning community.},
	language = {en},
	author = {Wassermann, Sarah and Cuvelier, Thibaut and Casas, Pedro},
	year = {2019},
	pages = {17},
	file = {Wassermann et al. - RAL - Improving Stream-Based Active Learning by Re.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/HJDHMIFA/Wassermann et al. - RAL - Improving Stream-Based Active Learning by Re.pdf:application/pdf}
}

@article{wassermann_adam_2019,
	title = {{ADAM} \& {RAL}: {Adaptive} {Memory} {Learning} and {Reinforcement} {Active} {Learning} for {Network} {Monitoring}},
	abstract = {Network-trafﬁc data commonly arrives in the form of fast data streams; online network-monitoring systems continuously analyze these kinds of streams, sequentially collecting measurements over time. Continuous and dynamic learning is an effective learning strategy when operating in these fast and dynamic environments, where concept drifts constantly occur. In this paper, we propose different approaches for stream-based machine learning, able to analyze network-trafﬁc streams on the ﬂy, using supervised learning techniques. We address two major challenges associated to stream-based machine learning and online network monitoring: (i) how to dynamically learn from and adapt to non-stationary data and patterns changing over time, and (ii) how to deal with the limited availability of ground truth or labeled data to continuously tune a supervised learning model. We introduce ADAM \& RAL, two stream-based machine-learning approaches to tackle these challenges. ADAM implements multiple stream-based machine-learning models and relies on an adaptive memory strategy to dynamically adapt the size of the system’s learning memory to the most recent data distribution, triggering new learning steps when concept drifts are detected. RAL implements a stream-based active-learning strategy to reduce the amount of labeled data needed for streambased learning, dynamically deciding on the most informative samples to integrate into the continuous learning scheme. Using a reinforcement learning loop, RAL improves prediction performance by additionally learning from the goodness of its previous sample-selection decisions. We focus on a particularly challenging problem in network monitoring: continuously tuning detection models able to recognize network attacks over time. By continuously learning from and detecting concept drifts within real network measurements, we show that ADAM \& RAL can continuously achieve high detection accuracy and limit the amount of training data needed to detect attacks over dynamic network data streams.},
	language = {en},
	author = {Wassermann, Sarah and Cuvelier, Thibaut and Mulinka, Pavol and Casas, Pedro},
	year = {2019},
	pages = {10},
	file = {Wassermann et al. - ADAM & RAL Adaptive Memory Learning and Reinforce.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/LQ9HU24M/Wassermann et al. - ADAM & RAL Adaptive Memory Learning and Reinforce.pdf:application/pdf}
}

@article{campos_skip_2018,
	title = {Skip {RNN}: {Learning} to {Skip} {State} {Updates} in {Recurrent} {Neural} {Networks}},
	shorttitle = {Skip {RNN}},
	url = {http://arxiv.org/abs/1708.06834},
	abstract = {Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/ .},
	urldate = {2020-01-21},
	journal = {arXiv:1708.06834 [cs]},
	author = {Campos, Victor and Jou, Brendan and Giro-i-Nieto, Xavier and Torres, Jordi and Chang, Shih-Fu},
	month = feb,
	year = {2018},
	note = {arXiv: 1708.06834},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted as conference paper at ICLR 2018},
	file = {arXiv Fulltext PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/NDQDYF8X/Campos et al. - 2018 - Skip RNN Learning to Skip State Updates in Recurr.pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/TXSEWRMW/1708.html:text/html}
}

@article{seo_neural_2018,
	title = {Neural {Speed} {Reading} via {Skim}-{RNN}},
	url = {http://arxiv.org/abs/1711.02085},
	abstract = {Inspired by the principles of speed reading, we introduce Skim-RNN, a recurrent neural network (RNN) that dynamically decides to update only a small fraction of the hidden state for relatively unimportant input tokens. Skim-RNN gives computational advantage over an RNN that always updates the entire hidden state. Skim-RNN uses the same input and output interfaces as a standard RNN and can be easily used instead of RNNs in existing models. In our experiments, we show that Skim-RNN can achieve significantly reduced computational cost without losing accuracy compared to standard RNNs across five different natural language tasks. In addition, we demonstrate that the trade-off between accuracy and speed of Skim-RNN can be dynamically controlled during inference time in a stable manner. Our analysis also shows that Skim-RNN running on a single CPU offers lower latency compared to standard RNNs on GPUs.},
	urldate = {2020-01-21},
	journal = {arXiv:1711.02085 [cs]},
	author = {Seo, Minjoon and Min, Sewon and Farhadi, Ali and Hajishirzi, Hannaneh},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.02085},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ICLR 2018},
	file = {arXiv Fulltext PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/BK7T9QEG/Seo et al. - 2018 - Neural Speed Reading via Skim-RNN.pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/TTMEA56F/1711.html:text/html}
}

@article{yu_learning_2017,
	title = {Learning to {Skim} {Text}},
	url = {http://arxiv.org/abs/1704.06877},
	abstract = {Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q{\textbackslash}\&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.},
	urldate = {2020-01-21},
	journal = {arXiv:1704.06877 [cs]},
	author = {Yu, Adams Wei and Lee, Hongrae and Le, Quoc V.},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.06877},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/3WSXFEAQ/Yu et al. - 2017 - Learning to Skim Text.pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/6PWJB5ZU/1704.html:text/html}
}

@article{gui_long_2018,
	title = {Long {Short}-{Term} {Memory} with {Dynamic} {Skip} {Connections}},
	url = {http://arxiv.org/abs/1811.03873},
	abstract = {In recent years, long short-term memory (LSTM) has been successfully used to model sequential data of variable length. However, LSTM can still experience difficulty in capturing long-term dependencies. In this work, we tried to alleviate this problem by introducing a dynamic skip connection, which can learn to directly connect two dependent words. Since there is no dependency information in the training data, we propose a novel reinforcement learning-based method to model the dependency relationship and connect dependent words. The proposed model computes the recurrent transition functions based on the skip connections, which provides a dynamic skipping advantage over RNNs that always tackle entire sentences sequentially. Our experimental results on three natural language processing tasks demonstrate that the proposed method can achieve better performance than existing methods. In the number prediction experiment, the proposed model outperformed LSTM with respect to accuracy by nearly 20\%.},
	urldate = {2020-01-21},
	journal = {arXiv:1811.03873 [cs]},
	author = {Gui, Tao and Zhang, Qi and Zhao, Lujun and Lin, Yaosong and Peng, Minlong and Gong, Jingjing and Huang, Xuanjing},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.03873},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/BQAEUSNQ/Gui et al. - 2018 - Long Short-Term Memory with Dynamic Skip Connectio.pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/CRG3WLSL/1811.html:text/html}
}

@article{hartl_explainability_2019,
	title = {Explainability and {Adversarial} {Robustness} for {RNNs}},
	url = {http://arxiv.org/abs/1912.09855},
	abstract = {Recurrent Neural Networks (RNNs) yield attractive properties for constructing Intrusion Detection Systems (IDSs) for network data. With the rise of ubiquitous Machine Learning (ML) systems, malicious actors have been catching up quickly to find new ways to exploit ML vulnerabilities for profit. Recently developed adversarial ML techniques focus on computer vision and their applicability to network traffic is not straightforward: Network packets expose fewer features than an image, are sequential and impose several constraints on their features. We show that despite these completely different characteristics, adversarial samples can be generated reliably for RNNs. To understand a classifier's potential for misclassification, we extend existing explainability techniques and propose new ones, suitable particularly for sequential data. Applying them shows that already the first packets of a communication flow are of crucial importance and are likely to be targeted by attackers. Feature importance methods show that even relatively unimportant features can be effectively abused to generate adversarial samples. Since traditional evaluation metrics such as accuracy are not sufficient for quantifying the adversarial threat, we propose the Adversarial Robustness Score (ARS) for comparing IDSs, capturing a common notion of adversarial robustness, and show that an adversarial training procedure can significantly and successfully reduce the attack surface.},
	urldate = {2020-01-21},
	journal = {arXiv:1912.09855 [cs, stat]},
	author = {Hartl, Alexander and Bachl, Maximilian and Fabini, Joachim and Zseby, Tanja},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.09855},
	keywords = {Computer Science - Networking and Internet Architecture, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/Q96GWYPR/Hartl et al. - 2019 - Explainability and Adversarial Robustness for RNNs.pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/UTLUVCAX/1912.html:text/html}
}

@article{meghdouri_analysis_2018,
	title = {Analysis of {Lightweight} {Feature} {Vectors} for {Attack} {Detection} in {Network} {Traffic}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	abstract = {The consolidation of encryption and big data in network communications have made deep packet inspection no longer feasible in large networks. Early attack detection requires feature vectors which are easy to extract, process, and analyze, allowing their generation also from encrypted traffic. So far, experts have selected features based on their intuition, previous research, or acritically assuming standards, but there is no general agreement about the features to use for attack detection in a broad scope. We compared five lightweight feature sets that have been proposed in the scientific literature for the last few years, and evaluated them with supervised machine learning. For our experiments, we use the UNSW-NB15 dataset, recently published as a new benchmark for network security. Results showed three remarkable findings: (1) Analysis based on source behavior instead of classic flow profiles is more effective for attack detection; (2) meta-studies on past research can be used to establish satisfactory benchmarks; and (3) features based on packet length are clearly determinant for capturing malicious activity. Our research showed that vectors currently used for attack detection are oversized, their accuracy and speed can be improved, and are to be adapted for dealing with encrypted traffic.},
	language = {en},
	number = {11},
	urldate = {2019-07-30},
	journal = {Applied Sciences},
	author = {Meghdouri, Fares and Zseby, Tanja and Iglesias, Félix},
	month = nov,
	year = {2018},
	keywords = {feature selection, network attack detection, supervised learning},
	pages = {2196}
}

@inproceedings{moustafa_unsw-nb15_2015,
	title = {{UNSW}-{NB15}: a comprehensive data set for network intrusion detection systems ({UNSW}-{NB15} network data set)},
	shorttitle = {{UNSW}-{NB15}},
	abstract = {One of the major research challenges in this field is the unavailability of a comprehensive network based data set which can reflect modern network traffic scenarios, vast varieties of low footprint intrusions and depth structured information about the network traffic. Evaluating network intrusion detection systems research efforts, KDD98, KDDCUP99 and NSLKDD benchmark data sets were generated a decade ago. However, numerous current studies showed that for the current network threat environment, these data sets do not inclusively reflect network traffic and modern low footprint attacks. Countering the unavailability of network benchmark data set challenges, this paper examines a UNSW-NB15 data set creation. This data set has a hybrid of the real modern normal and the contemporary synthesized attack activities of the network traffic. Existing and novel methods are utilised to generate the features of the UNSWNB15 data set. This data set is available for research purposes and can be accessed from the link.},
	booktitle = {{MilCIS}},
	author = {Moustafa, Nour and Slay, Jill},
	month = nov,
	year = {2015},
	keywords = {telecommunication traffic, Feature extraction, computer network security, IP networks, Servers, Benchmark testing, Data models, low footprint attacks, network intrusion detection systems, network traffic, NIDS, pcap files, Telecommunication traffic, testbed, Training, UNSW-NB15 data set, UNSW-NB15 network data set},
	pages = {1--6}
}

@inproceedings{sharafaldin_toward_2018,
	address = {Funchal, Madeira, Portugal},
	title = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}},
	shorttitle = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}},
	abstract = {Intrusion Detection, IDS Dataset, DoS, Web Attack, Inﬁltration, Brute Force.},
	language = {en},
	urldate = {2019-09-10},
	booktitle = {{ICISSP}},
	publisher = {SCITEPRESS},
	author = {Sharafaldin, Iman and Habibi Lashkari, Arash and Ghorbani, Ali A.},
	year = {2018},
	pages = {108--116}
}

@inproceedings{bachl_walling_2019,
	address = {Orlando, FL, USA},
	title = {Walling {Up} {Backdoors} in {Intrusion} {Detection} {Systems}},
	abstract = {Interest in poisoning attacks and backdoors recently resurfaced for Deep Learning (DL) applications. Several successful defense mechanisms have been recently proposed for Convolutional Neural Networks (CNNs), for example in the context of autonomous driving. We show that visualization approaches can aid in identifying a backdoor independent of the used classifier. Surprisingly, we find that common defense mechanisms fail utterly to remove backdoors in DL for Intrusion Detection Systems (IDSs). Finally, we devise pruning-based approaches to remove backdoors for Decision Trees (DTs) and Random Forests (RFs) and demonstrate their effectiveness for two different network security datasets.},
	urldate = {2019-12-07},
	booktitle = {Big-{DAMA} '19},
	publisher = {ACM},
	author = {Bachl, Maximilian and Hartl, Alexander and Fabini, Joachim and Zseby, Tanja},
	year = {2019},
	keywords = {Pruning, Deep Learning, Explainable AI, Network security, Poisoning attack, Random Forests},
	pages = {8--13}
}

@inproceedings{bachl_rax_2019,
	title = {Rax: {Deep} {Reinforcement} {Learning} for {Congestion} {Control}},
	shorttitle = {Rax},
	doi = {10.1109/ICC.2019.8761187},
	abstract = {This paper proposes Reactive Adaptive eXperience based congestion control (Rax), a new method of congestion control (CC) that uses online reinforcement learning (RL) to maintain an optimum congestion window with respect to a given reward function and based on current network conditions. We use a neural network based approach that can be initialized either with random weights or with a previously trained neural network to improve stability and convergence time. As the processing of rewards in CC depends on the arrival of acknowledgements, which are delayed and received one by one, the problem is not suitable for current implementations of Deep RL. As a remedy we propose Partial Action Learning, a formulation of Deep RL that supports delayed and partial rewards. We show that our method converges to a stable, close-to-optimum solution within minutes and outperforms existing CC algorithms in typical networks. Thus, this paper demonstrates that Deep RL can be done online and can compete with classic CC schemes such as Cubic.},
	booktitle = {{ICC} 2019 - 2019 {IEEE} {International} {Conference} on {Communications} ({ICC})},
	author = {Bachl, Maximilian and Zseby, Tanja and Fabini, Joachim},
	month = may,
	year = {2019},
	note = {ISSN: 1550-3607},
	keywords = {Reinforcement learning, learning (artificial intelligence), telecommunication congestion control, neural nets, neural network, Measurement, Biological neural networks, convergence time, deep reinforcement learning, deep RL, Microsoft Windows, online reinforcement learning, optimum congestion window, partial action learning, Rax, reactive adaptive experience based congestion control, reward function, telecommunication computing, Telecommunications, TV},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/PZGRMM59/8761187.html:text/html;IEEE Xplore Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/JTG9DWLB/Bachl et al. - 2019 - Rax Deep Reinforcement Learning for Congestion Co.pdf:application/pdf}
}

@article{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the ﬁrst deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We ﬁnd that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	language = {en},
	urldate = {2020-01-24},
	journal = {arXiv:1312.5602 [cs]},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.5602},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: NIPS Deep Learning Workshop 2013},
	file = {Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/GMYI3I5J/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf}
}

@inproceedings{schwartz_reinforcement_1993,
	title = {A reinforcement learning method for maximizing undiscounted rewards},
	volume = {298},
	booktitle = {Proceedings of the tenth international conference on machine learning},
	author = {Schwartz, Anton},
	year = {1993},
	pages = {298--305}
}

@inproceedings{mirsky_kitsune_2018,
	address = {San Diego, CA},
	title = {Kitsune: {An} {Ensemble} of {Autoencoders} for {Online} {Network} {Intrusion} {Detection}},
	isbn = {978-1-891562-49-5},
	shorttitle = {Kitsune},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_03A-3_Mirsky_paper.pdf},
	doi = {10.14722/ndss.2018.23204},
	abstract = {Neural networks have become an increasingly popular solution for network intrusion detection systems (NIDS). Their capability of learning complex patterns and behaviors make them a suitable solution for differentiating between normal trafﬁc and network attacks. However, a drawback of neural networks is the amount of resources needed to train them. Many network gateways and routers devices, which could potentially host an NIDS, simply do not have the memory or processing power to train and sometimes even execute such models. More importantly, the existing neural network solutions are trained in a supervised manner. Meaning that an expert must label the network trafﬁc and update the model manually from time to time.},
	language = {en},
	urldate = {2020-01-24},
	booktitle = {Proceedings 2018 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Mirsky, Yisroel and Doitshman, Tomer and Elovici, Yuval and Shabtai, Asaf},
	year = {2018},
	file = {Mirsky et al. - 2018 - Kitsune An Ensemble of Autoencoders for Online Ne.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/VTGHGQ3R/Mirsky et al. - 2018 - Kitsune An Ensemble of Autoencoders for Online Ne.pdf:application/pdf}
}

@inproceedings{cho_learning_2014,
	address = {Doha, Qatar},
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}–{Decoder} for {Statistical} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/D14-1179},
	doi = {10.3115/v1/D14-1179},
	urldate = {2020-02-04},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Kyunghyun and van Merriënboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = oct,
	year = {2014},
	pages = {1724--1734},
	file = {Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/7CAFFWTD/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder–.pdf:application/pdf}
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	number = {8},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	pages = {1735--1780}
}